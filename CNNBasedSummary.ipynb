{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNBasedSummary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1mzd8wnFs7i2_07yMWkdIQu3SlACfickw",
      "authorship_tag": "ABX9TyNTIaiQThB/G1TeuRW+E/81",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varshaelza/Text-Summarization-Categorization/blob/main/CNNBasedSummary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roCiKAKdZPpV"
      },
      "source": [
        "Sentence to Salience"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVWdilXyYXIp"
      },
      "source": [
        "import pandas as pd\n",
        "t = pd.read_csv('/content/drive/MyDrive/BBC Dataset/LDA Summarization/LDASummariestrain.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJepYOCCYYxa"
      },
      "source": [
        "t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5Pci52UYgDG"
      },
      "source": [
        "rawData = {}\n",
        "rawSummaries = {}\n",
        "    \n",
        "# running count variable -- keeps track of the total size\n",
        "totalSentences = 0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0W1rSKsYkAK"
      },
      "source": [
        "def sep(text):\n",
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "  import re\n",
        "  text= text.replace(\"\\t\",\" \")\n",
        "  text=re.sub(r'\\.+', '.',text)\n",
        "  k=0\n",
        "  c=0\n",
        "  p=0\n",
        "  a_list=[]\n",
        "  while k<len(text):\n",
        "    if c==0 and text[k]=='.':\n",
        "      a_list.append(text[p:k+1])\n",
        "      p=k+1\n",
        "    elif c==0 and text[k]=='\"':\n",
        "      c=1\n",
        "    elif c==1 and text[k]=='\"' and text[k-1]==\".\" :\n",
        "      a_list.append(text[p:k+1])\n",
        "      p=k+1\n",
        "      c=0\n",
        "    elif c==1 and text[k]=='\"' and text[k-2]==\".\" :\n",
        "      a_list.append(text[p:k+1])\n",
        "      p=k+1\n",
        "      c=0\n",
        "    elif c==1 and text[k]=='\"':\n",
        "      c=0\n",
        "    k=k+1\n",
        "  if p<len(text)-1:\n",
        "    a_list.append(text[p:len(text)])\n",
        "  return a_list\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_MDtu7pYoKA"
      },
      "source": [
        "def spl(text):\n",
        "  t=text.split('\\n',1)\n",
        "  if len(t)!=2:\n",
        "    return \"\"\n",
        "  else :\n",
        "    return t[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhHoXRxEYxGM"
      },
      "source": [
        "j=0\n",
        "while j<len(t):\n",
        "  a=sep(t['News'][j])\n",
        "  a[0]=spl(a[0])\n",
        "  a[0]=spl(a[0])\n",
        "  if a[0]==\"\":\n",
        "    del a[0]\n",
        "  i=0\n",
        "  while i<len(a):\n",
        "    a[i] = a[i].replace('\\n',' ')\n",
        "    a[i] = a[i].replace('\\t',' ')\n",
        "    a[i]= a[i].replace(r'\\.+', \".\")\n",
        "    a[i]= a[i].replace(r'\\ +', \" \")\n",
        "    i=i+1\n",
        "\n",
        "  b=sep(t['Summary'][j])\n",
        "  b[0]=spl(b[0])\n",
        "  b[0]=spl(b[0])\n",
        "  if b[0]==\"\":\n",
        "    del b[0]\n",
        "  i=0\n",
        "  while i<len(b):\n",
        "    b[i] = b[i].replace('\\n',' ')\n",
        "    b[i] = b[i].replace('\\t',' ')\n",
        "    b[i]= b[i].replace(r'\\.+', \".\")\n",
        "    b[i]= b[i].replace(r'\\ +', \" \")\n",
        "    i=i+1\n",
        "  \n",
        "  rawData[j]=a\n",
        "  rawSummaries[j]=b\n",
        "  j=j+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXMG5asyYxyu"
      },
      "source": [
        "!pip install rouge/requirements.txt\n",
        "!pip install rouge-score\n",
        "import numpy as np\n",
        "\n",
        "def calc_rouge_scores(pred_summaries, gold_summaries, \n",
        "                                 keys=['rouge1', 'rouge2'], use_stemmer=True):\n",
        "    #Calculate rouge scores\n",
        "    from rouge_score import rouge_scorer\n",
        "    scorer = rouge_scorer.RougeScorer(keys, use_stemmer= use_stemmer)\n",
        "    n = len(pred_summaries)\n",
        "    scores = [scorer.score(pred_summaries[j], gold_summaries[j]) for \n",
        "              j in range(n)] \n",
        "              \n",
        "    #create dict\n",
        "    dict_scores={}                                                            \n",
        "    for key in keys:\n",
        "        dict_scores.update({key: {}})\n",
        "        \n",
        "    #populate dict    \n",
        "    for key in keys:\n",
        "        \n",
        "        precision_list = [scores[j][key][0] for j in range(len(scores))]\n",
        "        recall_list = [scores[j][key][1] for j in range(len(scores))]\n",
        "        f1_list = [scores[j][key][2] for j in range(len(scores))]\n",
        "\n",
        "        precision = np.mean(precision_list)\n",
        "        recall = np.mean(recall_list)\n",
        "        f1 = np.mean(f1_list)\n",
        "        \n",
        "        dict_results = {'recall': recall, 'precision': precision, 'f1': f1}\n",
        "        \n",
        "        dict_scores[key] = dict_results\n",
        "        \n",
        "    return dict_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jau-jB6zY4_m"
      },
      "source": [
        "saliency={}\n",
        "i=0\n",
        "while i<len(rawData):\n",
        "  saliency[i]=[]\n",
        "  j=0\n",
        "  while j<len(rawData[i]):\n",
        "    s=calc_rouge_scores([rawData[i][j]],[t['Summary'][i]])\n",
        "    saliency[i].append(0.5*s['rouge1']['f1']+s['rouge2']['f1']*0.5) #alpha=0.5\n",
        "    j=j+1\n",
        "  i=i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwDJxU9fY5uo"
      },
      "source": [
        "tot=0;\n",
        "i=0\n",
        "while i<len(rawData):\n",
        "  tot=tot+len(rawData[i])\n",
        "  i=i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cLBJXe9ZAJn"
      },
      "source": [
        "import numpy as np\n",
        "nx3output = np.zeros((tot, 3), dtype=object)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME-MVOxzYx3n"
      },
      "source": [
        "i=0\n",
        "c=0\n",
        "while i<len(rawData):\n",
        "  j=0\n",
        "  while j<len(rawData[i]):\n",
        "    nx3output[c,0]=i\n",
        "    nx3output[c,1]=rawData[i][j]\n",
        "    nx3output[c,2]=saliency[i][j]\n",
        "    j=j+1\n",
        "    c=c+1\n",
        "  i=i+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKHoIrtQZF7I"
      },
      "source": [
        "import pickle\n",
        "f = open(\"/content/drive/MyDrive/BBC Dataset/CNN Summarisation/sentencesToSaliency.pickle\", \"wb\")\n",
        "pickle.dump(nx3output, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVQTu6gHZJBT"
      },
      "source": [
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei8JnzyeZLTg"
      },
      "source": [
        "t.to_csv('/content/drive/MyDrive/BBC Dataset/CNN Summarisation/CNNSummariestrain')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP4EVeW-nqeG"
      },
      "source": [
        "Embedding Sentences ith Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3hWQ3JmqJuR",
        "outputId": "199ca737-dd29-406d-fc83-4066c778b820"
      },
      "source": [
        "!cd word2vec/\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: cd: word2vec/: No such file or directory\n",
            "--2021-05-06 14:39:49--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.228.35\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.228.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  32.7MB/s    in 49s     \n",
            "\n",
            "2021-05-06 14:40:39 (32.0 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL9H1vP-oSn_"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqy6_kMlnua3"
      },
      "source": [
        "def loadFromPickle(fileName):\n",
        "    f = open(fileName, \"rb\")\n",
        "    data = pickle.load(f)\n",
        "    f.close()\n",
        "    return data "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_Y7UwtrnyRz"
      },
      "source": [
        "data = loadFromPickle(\"/content/drive/MyDrive/BBC Dataset/CNN Summarisation/sentencesToSaliency.pickle\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEEe13-MjeFg"
      },
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#def embed_sentences(data, word2vec_limit = 50000 , NUM_WORDS=20000):"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AchAX70ohzW"
      },
      "source": [
        "def embed_sentences(data, word2vec_limit = 50000 , NUM_WORDS=20000):   \n",
        "    '''\n",
        "    Embed sentences\n",
        "    Params:\n",
        "        data             - np.array  [ doc id, sentences, saliency score ]\n",
        "                            \n",
        "        word2vec_limit   - int: number of words used in the word embedding provided by Google\n",
        "                            - ex: 50000\n",
        "        NUM_WORDS        - int: The maximum number of words to keep, based on word frequency. Only the most common num_words words will be kept.\n",
        "                            - ex: 20000\n",
        "       \n",
        "    Returns:\n",
        "        input_output        - np.array [embedding matrix , saliency score]\n",
        "                        \n",
        "    '''\n",
        "    \n",
        "    #setences ex: [\"It's the first sentence!\",\"it is the second sentence\"]\n",
        "    sentences = data[:,1]\n",
        "    \n",
        "    #Load Google pre-trained words as a model\n",
        "    embedding_model = gensim.models.KeyedVectors.load_word2vec_format('/content/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=word2vec_limit)\n",
        "    #Convert the model as a dictionnary word_vectors[\"hello\"] will return a vector like [0.3, 3, ... , -4]\n",
        "    word_vectors = embedding_model\n",
        "    #print(\"Embedding for 'hello': \", word_vectors[\"hello\"], \"\\n\")\n",
        "    \n",
        "    # Tokenize the sentences, that is to say convert the 2 sentences [\"It's the first sentence!\",\"It is the second sentence\"] to 2 sequences [[1 4 2 5 3],[1 6 2 7 3]]\n",
        "    # It handles the ennoying cases (punctuation, Upper cases, etc...)\n",
        "    tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',lower=True)\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    padded_sequences = pad_sequences(sequences)\n",
        "    #print(\"Padded_sequences: \", padded_sequences, \"\\n\")   \n",
        "    \n",
        "    #Produce a dictionnary mapping words to tokens e.g. {'it': 1, 'the': 2, 'sentence': 3, 's': 4, 'first': 5, 'is': 6, 'second': 7}\n",
        "    word_index = tokenizer.word_index\n",
        "    #print(\"word_index: \" , word_index , \"\\n\" )\n",
        "    \n",
        "    #Build a dictionnary mapping tokens to vectors e.g. {'1': [2, ... , -3] ; '2': [4, ... , 0.8] ; ... }\n",
        "    embedding_weights = {key: embedding_model[word] if word in word_vectors.vocab else\n",
        "                              np.random.uniform(-0.25, 0.25, word_vectors.vector_size)\n",
        "                        for word, key in word_index.items()}\n",
        "    # Add the token \"0\", used for padding\n",
        "    embedding_weights[0] = np.zeros(word_vectors.vector_size)\n",
        "   \n",
        "    #print(\"Embedding weights: \" , embedding_weights , \"\\n\")\n",
        "    \n",
        "    #Build a 3D array: 1D for the sentences, 1D for the words and 1D for the word2vec dimensions. \n",
        "    embedded_sentences = np.stack([np.stack([embedding_weights[token] for token in sentence]) for sentence in padded_sequences])\n",
        "    \n",
        "    #Add back the saliency scores\n",
        "    #input_output = np.column_stack((embedded_sentences,data[:,2]))    \n",
        "    \n",
        "    input_output = np.array([])\n",
        "    for i in range(len(data)):\n",
        "        input_output = np.append(input_output,np.array([ embedded_sentences[i] , data[i,2] ]) )\n",
        "        \n",
        "    del embedding_model\n",
        "    \n",
        "    return input_output"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIKhxH5wIokP",
        "outputId": "f52bcf68-e7dd-4c6e-b1c7-5fa815f83373"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31592"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhmY0sWdItfL",
        "outputId": "77e77f08-410e-4871-ebb4-f98e83aab49d"
      },
      "source": [
        "data1=data[0:3000]\n",
        "data1=embed_sentences(data1)\n",
        "f = open('/content/drive/MyDrive/BBC Dataset/CNN Summarisation/embeddingsToSaliency1.pickle', \"wb\")\n",
        "pickle.dump(data1,f)\n",
        " "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGeUzjz4MDTP",
        "outputId": "d75fc4cd-0ecf-4559-b5d6-8872723f3147"
      },
      "source": [
        "data1=data[3000:4000]\n",
        "data1=embed_sentences(data1)\n",
        "f = open('embeddingsToSaliency2.pickle', \"wb\")\n",
        "pickle.dump(data1,f)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C1gNaQsQ747",
        "outputId": "7cb652ad-f2a0-4fa5-c0d8-fb7e301152d2"
      },
      "source": [
        "data1=data[4000:5000]\n",
        "data1=embed_sentences(data1)\n",
        "f = open('embeddingsToSaliency2a.pickle', \"wb\")\n",
        "pickle.dump(data1,f)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcIuh_91R8Bz",
        "outputId": "58ad5c5f-0505-4678-ad59-92673e59d78e"
      },
      "source": [
        "data1=data[5000:6000]\n",
        "data1=embed_sentences(data1)\n",
        "f = open('embeddingsToSaliency2b.pickle', \"wb\")\n",
        "pickle.dump(data1,f)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgeXoE1whOHy",
        "outputId": "40662f0e-4770-4e50-e661-0acc50a55601"
      },
      "source": [
        "data1=data[6000:9000]\n",
        "data1=embed_sentences(data1)\n",
        "f = open('/content/drive/MyDrive/BBC Dataset/CNN Summarisation/embeddingsToSaliency3.pickle', \"wb\")\n",
        "pickle.dump(data1,f)\n",
        " \n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9opI5ICFJ7-j",
        "outputId": "e76756cc-8539-4dee-ea20-4677c26ba292"
      },
      "source": [
        "data1=data[9000:12000]\n",
        "data1=embed_sentences(data1)\n",
        "f = open('/content/drive/MyDrive/BBC Dataset/CNN Summarisation/embeddingsToSaliency4.pickle', \"wb\")\n",
        "pickle.dump(data1,f)\n",
        " "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNebzzHaJ_v7",
        "outputId": "43ff83bc-c9db-4e8f-d9cd-8636ca66cdfc"
      },
      "source": [
        "data1=data[12000:15000]\n",
        "data1=embed_sentences(data1)\n",
        "f = open('/content/drive/MyDrive/BBC Dataset/CNN Summarisation/embeddingsToSaliency5.pickle', \"wb\")\n",
        "pickle.dump(data1,f)\n",
        " "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjH2APg-KHg5",
        "outputId": "110839a6-b306-4765-c625-28f8830626e1"
      },
      "source": [
        "data1=data[15000:17000]\n",
        "data1=embed_sentences(data1)\n",
        "f = open('embeddingsToSaliency6.pickle', \"wb\")\n",
        "pickle.dump(data1,f)\n",
        " "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdArnhCNQmXq",
        "outputId": "67da5c0e-4f2c-4983-e3b3-d39db491426e"
      },
      "source": [
        "data1=data[17000:18000]\n",
        "data1=embed_sentences(data1)\n",
        "f = open('embeddingsToSaliency6a.pickle', \"wb\")\n",
        "pickle.dump(data1,f)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao5_H2nuKTd6",
        "outputId": "20461da6-872a-4fd0-f868-4634851cca25"
      },
      "source": [
        "data1=data[18000:21000]\n",
        "data1=embed_sentences(data1)\n",
        "f = open('/content/drive/MyDrive/BBC Dataset/CNN Summarisation/embeddingsToSaliency7.pickle', \"wb\")\n",
        "pickle.dump(data1,f)\n",
        " "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQXy3JGHKeb9",
        "outputId": "95f0c01d-4fb1-48dc-f946-22777a1c0a36"
      },
      "source": [
        "data1=data[21000:24000]\n",
        "data1=embed_sentences(data1)\n",
        "f = open('/content/drive/MyDrive/BBC Dataset/CNN Summarisation/embeddingsToSaliency8.pickle', \"wb\")\n",
        "pickle.dump(data1,f)\n",
        " "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIz1gYIFKh5k",
        "outputId": "34157afa-1423-49aa-98e2-5f766fc76359"
      },
      "source": [
        "data1=data[24000:27000]\n",
        "data1=embed_sentences(data1)\n",
        "f = open('/content/drive/MyDrive/BBC Dataset/CNN Summarisation/embeddingsToSaliency9.pickle', \"wb\")\n",
        "pickle.dump(data1,f)\n",
        " "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDwhgCOyKkUK",
        "outputId": "688a1ccc-0942-427e-8e64-ad72ca1c8dae"
      },
      "source": [
        "data1=data[27000:29000]\n",
        "data1=embed_sentences(data1)\n",
        "f = open('embeddingsToSaliency10.pickle', \"wb\")\n",
        "pickle.dump(data1,f)\n",
        " "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeEE-MuoQO35",
        "outputId": "c19ffcfd-31c2-41f6-eabd-dc3e3d1ab8c8"
      },
      "source": [
        "data1=data[29000:30000]\n",
        "data1=embed_sentences(data1)\n",
        "f = open('embeddingsToSaliency10a.pickle', \"wb\")\n",
        "pickle.dump(data1,f)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcjG50OaKq5q",
        "outputId": "e0bdd384-74ec-4ea5-ab42-15128ff7ba6e"
      },
      "source": [
        "data1=data[30000:len(data)]\n",
        "data1=embed_sentences(data1)\n",
        "f = open('embeddingsToSaliency11.pickle', \"wb\")\n",
        "pickle.dump(data1,f)\n",
        " "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vxFWQGyI0Ki"
      },
      "source": [
        "#2 2a 2b 6 6a 10 10a 11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6ivmVo_mGRn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}