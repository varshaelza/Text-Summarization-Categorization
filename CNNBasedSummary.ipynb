{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNBasedSummary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1mzd8wnFs7i2_07yMWkdIQu3SlACfickw",
      "authorship_tag": "ABX9TyO7+84SuEAB+xGWNGqSmVLz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varshaelza/Text-Summarization-Categorization/blob/main/CNNBasedSummary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roCiKAKdZPpV"
      },
      "source": [
        "Sentence to Salience"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVWdilXyYXIp"
      },
      "source": [
        "import pandas as pd\n",
        "t = pd.read_csv('/content/drive/MyDrive/BBC Dataset/LDA Summarization/LDASummariestrain.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJepYOCCYYxa"
      },
      "source": [
        "t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5Pci52UYgDG"
      },
      "source": [
        "rawData = {}\n",
        "rawSummaries = {}\n",
        "    \n",
        "# running count variable -- keeps track of the total size\n",
        "totalSentences = 0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0W1rSKsYkAK"
      },
      "source": [
        "def sep(text):\n",
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "  import re\n",
        "  text= text.replace(\"\\t\",\" \")\n",
        "  text=re.sub(r'\\.+', '.',text)\n",
        "  k=0\n",
        "  c=0\n",
        "  p=0\n",
        "  a_list=[]\n",
        "  while k<len(text):\n",
        "    if c==0 and text[k]=='.':\n",
        "      a_list.append(text[p:k+1])\n",
        "      p=k+1\n",
        "    elif c==0 and text[k]=='\"':\n",
        "      c=1\n",
        "    elif c==1 and text[k]=='\"' and text[k-1]==\".\" :\n",
        "      a_list.append(text[p:k+1])\n",
        "      p=k+1\n",
        "      c=0\n",
        "    elif c==1 and text[k]=='\"' and text[k-2]==\".\" :\n",
        "      a_list.append(text[p:k+1])\n",
        "      p=k+1\n",
        "      c=0\n",
        "    elif c==1 and text[k]=='\"':\n",
        "      c=0\n",
        "    k=k+1\n",
        "  if p<len(text)-1:\n",
        "    a_list.append(text[p:len(text)])\n",
        "  return a_list\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_MDtu7pYoKA"
      },
      "source": [
        "def spl(text):\n",
        "  t=text.split('\\n',1)\n",
        "  if len(t)!=2:\n",
        "    return \"\"\n",
        "  else :\n",
        "    return t[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhHoXRxEYxGM"
      },
      "source": [
        "j=0\n",
        "while j<len(t):\n",
        "  a=sep(t['News'][j])\n",
        "  a[0]=spl(a[0])\n",
        "  a[0]=spl(a[0])\n",
        "  if a[0]==\"\":\n",
        "    del a[0]\n",
        "  i=0\n",
        "  while i<len(a):\n",
        "    a[i] = a[i].replace('\\n',' ')\n",
        "    a[i] = a[i].replace('\\t',' ')\n",
        "    a[i]= a[i].replace(r'\\.+', \".\")\n",
        "    a[i]= a[i].replace(r'\\ +', \" \")\n",
        "    i=i+1\n",
        "\n",
        "  b=sep(t['Summary'][j])\n",
        "  b[0]=spl(b[0])\n",
        "  b[0]=spl(b[0])\n",
        "  if b[0]==\"\":\n",
        "    del b[0]\n",
        "  i=0\n",
        "  while i<len(b):\n",
        "    b[i] = b[i].replace('\\n',' ')\n",
        "    b[i] = b[i].replace('\\t',' ')\n",
        "    b[i]= b[i].replace(r'\\.+', \".\")\n",
        "    b[i]= b[i].replace(r'\\ +', \" \")\n",
        "    i=i+1\n",
        "  \n",
        "  rawData[j]=a\n",
        "  rawSummaries[j]=b\n",
        "  j=j+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXMG5asyYxyu"
      },
      "source": [
        "!pip install rouge/requirements.txt\n",
        "!pip install rouge-score\n",
        "import numpy as np\n",
        "\n",
        "def calc_rouge_scores(pred_summaries, gold_summaries, \n",
        "                                 keys=['rouge1', 'rouge2'], use_stemmer=True):\n",
        "    #Calculate rouge scores\n",
        "    from rouge_score import rouge_scorer\n",
        "    scorer = rouge_scorer.RougeScorer(keys, use_stemmer= use_stemmer)\n",
        "    n = len(pred_summaries)\n",
        "    scores = [scorer.score(pred_summaries[j], gold_summaries[j]) for \n",
        "              j in range(n)] \n",
        "              \n",
        "    #create dict\n",
        "    dict_scores={}                                                            \n",
        "    for key in keys:\n",
        "        dict_scores.update({key: {}})\n",
        "        \n",
        "    #populate dict    \n",
        "    for key in keys:\n",
        "        \n",
        "        precision_list = [scores[j][key][0] for j in range(len(scores))]\n",
        "        recall_list = [scores[j][key][1] for j in range(len(scores))]\n",
        "        f1_list = [scores[j][key][2] for j in range(len(scores))]\n",
        "\n",
        "        precision = np.mean(precision_list)\n",
        "        recall = np.mean(recall_list)\n",
        "        f1 = np.mean(f1_list)\n",
        "        \n",
        "        dict_results = {'recall': recall, 'precision': precision, 'f1': f1}\n",
        "        \n",
        "        dict_scores[key] = dict_results\n",
        "        \n",
        "    return dict_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jau-jB6zY4_m"
      },
      "source": [
        "saliency={}\n",
        "i=0\n",
        "while i<len(rawData):\n",
        "  saliency[i]=[]\n",
        "  j=0\n",
        "  while j<len(rawData[i]):\n",
        "    s=calc_rouge_scores([rawData[i][j]],[t['Summary'][i]])\n",
        "    saliency[i].append(0.5*s['rouge1']['f1']+s['rouge2']['f1']*0.5) #alpha=0.5\n",
        "    j=j+1\n",
        "  i=i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwDJxU9fY5uo"
      },
      "source": [
        "tot=0;\n",
        "i=0\n",
        "while i<len(rawData):\n",
        "  tot=tot+len(rawData[i])\n",
        "  i=i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cLBJXe9ZAJn"
      },
      "source": [
        "import numpy as np\n",
        "nx3output = np.zeros((tot, 3), dtype=object)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME-MVOxzYx3n"
      },
      "source": [
        "i=0\n",
        "c=0\n",
        "while i<len(rawData):\n",
        "  j=0\n",
        "  while j<len(rawData[i]):\n",
        "    nx3output[c,0]=i\n",
        "    nx3output[c,1]=rawData[i][j]\n",
        "    nx3output[c,2]=saliency[i][j]\n",
        "    j=j+1\n",
        "    c=c+1\n",
        "  i=i+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKHoIrtQZF7I"
      },
      "source": [
        "import pickle\n",
        "f = open(\"/content/drive/MyDrive/BBC Dataset/CNN Summarisation/sentencesToSaliency.pickle\", \"wb\")\n",
        "pickle.dump(nx3output, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVQTu6gHZJBT"
      },
      "source": [
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei8JnzyeZLTg"
      },
      "source": [
        "t.to_csv('/content/drive/MyDrive/BBC Dataset/CNN Summarisation/CNNSummariestrain')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP4EVeW-nqeG"
      },
      "source": [
        "Embedding Sentences ith Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3hWQ3JmqJuR",
        "outputId": "cfa313e3-45e3-4966-8b61-d2eed33f7650"
      },
      "source": [
        "!cd word2vec/\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: cd: word2vec/: No such file or directory\n",
            "--2021-05-07 11:34:49--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.130.184\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.130.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL9H1vP-oSn_"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqy6_kMlnua3"
      },
      "source": [
        "def loadFromPickle(fileName):\n",
        "    f = open(fileName, \"rb\")\n",
        "    data = pickle.load(f)\n",
        "    f.close()\n",
        "    return data "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_Y7UwtrnyRz"
      },
      "source": [
        "data = loadFromPickle(\"/content/drive/MyDrive/BBC Dataset/CNN Summarisation/sentencesToSaliency.pickle\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEEe13-MjeFg"
      },
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#def embed_sentences(data, word2vec_limit = 50000 , NUM_WORDS=20000):"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AchAX70ohzW"
      },
      "source": [
        "def embed_sentences(data, word2vec_limit = 50000 , NUM_WORDS=20000):   \n",
        "    '''\n",
        "    Embed sentences\n",
        "    Params:\n",
        "        data             - np.array  [ doc id, sentences, saliency score ]\n",
        "                            \n",
        "        word2vec_limit   - int: number of words used in the word embedding provided by Google\n",
        "                            - ex: 50000\n",
        "        NUM_WORDS        - int: The maximum number of words to keep, based on word frequency. Only the most common num_words words will be kept.\n",
        "                            - ex: 20000\n",
        "       \n",
        "    Returns:\n",
        "        input_output        - np.array [embedding matrix , saliency score]\n",
        "                        \n",
        "    '''\n",
        "    i=0\n",
        "    k=1\n",
        "    while i<2500:\n",
        "      #setences ex: [\"It's the first sentence!\",\"it is the second sentence\"]\n",
        "      sentences = data[:,1]\n",
        "      \n",
        "      #Load Google pre-trained words as a model\n",
        "      embedding_model = gensim.models.KeyedVectors.load_word2vec_format('/content/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=word2vec_limit)\n",
        "      #Convert the model as a dictionnary word_vectors[\"hello\"] will return a vector like [0.3, 3, ... , -4]\n",
        "      word_vectors = embedding_model\n",
        "      #print(\"Embedding for 'hello': \", word_vectors[\"hello\"], \"\\n\")\n",
        "      \n",
        "      # Tokenize the sentences, that is to say convert the 2 sentences [\"It's the first sentence!\",\"It is the second sentence\"] to 2 sequences [[1 4 2 5 3],[1 6 2 7 3]]\n",
        "      # It handles the ennoying cases (punctuation, Upper cases, etc...)\n",
        "      tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',lower=True)\n",
        "      tokenizer.fit_on_texts(sentences)\n",
        "      sequences = tokenizer.texts_to_sequences(sentences)\n",
        "      padded_sequences = pad_sequences(sequences)\n",
        "      #print(\"Padded_sequences: \", padded_sequences, \"\\n\")   \n",
        "      \n",
        "      #Produce a dictionnary mapping words to tokens e.g. {'it': 1, 'the': 2, 'sentence': 3, 's': 4, 'first': 5, 'is': 6, 'second': 7}\n",
        "      word_index = tokenizer.word_index\n",
        "      #print(\"word_index: \" , word_index , \"\\n\" )\n",
        "      \n",
        "      #Build a dictionnary mapping tokens to vectors e.g. {'1': [2, ... , -3] ; '2': [4, ... , 0.8] ; ... }\n",
        "      embedding_weights = {key: embedding_model[word] if word in word_vectors.vocab else\n",
        "                                np.random.uniform(-0.25, 0.25, word_vectors.vector_size)\n",
        "                          for word, key in word_index.items()}\n",
        "      # Add the token \"0\", used for padding\n",
        "      embedding_weights[0] = np.zeros(word_vectors.vector_size)\n",
        "    \n",
        "      #print(\"Embedding weights: \" , embedding_weights , \"\\n\")\n",
        "      \n",
        "      #Build a 3D array: 1D for the sentences, 1D for the words and 1D for the word2vec dimensions.\n",
        "      print('hey1')\n",
        "      del sequences\n",
        "      del embedding_model\n",
        "      p= padded_sequences[i:i+500]\n",
        "      del padded_sequences\n",
        "      print('hey2')\n",
        "      embedded_sentences = np.stack([np.stack([embedding_weights[token] for token in sentence]) for sentence in p])\n",
        "      print('hey3')\n",
        "      del p\n",
        "      del embedding_weights\n",
        "      i=i+500\n",
        "      input_output = np.array([])\n",
        "      j=i-500\n",
        "      while j<i:\n",
        "        input_output = np.append(input_output,np.array([ embedded_sentences[j-(i-500)] , data[j,2] ]) )\n",
        "        j=j+1\n",
        "      f = open('/content/drive/MyDrive/BBC Dataset/CNN Summarisation/embeddingsToSaliency'+str(k)+'.pickle', \"wb\")\n",
        "      pickle.dump(input_output,f)\n",
        "      \n",
        "      k=k+1\n",
        "      \n",
        "      del embedded_sentences\n",
        "      del input_output\n",
        "\n",
        "    \n",
        "      #Add back the saliency scores\n",
        "      #input_output = np.column_stack((embedded_sentences,data[:,2]))    \n",
        "      \n",
        "      #input_output = np.array([])\n",
        "      #for i in range(len(data)):\n",
        "      #   input_output = np.append(input_output,np.array([ embedded_sentences[i] , data[i,2] ]) )\n",
        "          \n",
        "      \n",
        "      \n",
        "      #return input_output"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kldDwg_loJoP",
        "outputId": "e8a4df3c-9598-4bc2-db51-64903b18d53e"
      },
      "source": [
        "embed_sentences(data)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hey1\n",
            "hey2\n",
            "hey3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "hey1\n",
            "hey2\n",
            "hey3\n",
            "hey1\n",
            "hey2\n",
            "hey3\n",
            "hey1\n",
            "hey2\n",
            "hey3\n",
            "hey1\n",
            "hey2\n",
            "hey3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmyKO42sulJO"
      },
      "source": [
        "CNN Model Bilding and training using Keras.Training is done in stages due to large size of Word2Vec  embeddings of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBQYs6jBwfzC"
      },
      "source": [
        "import pickle\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATl7eTj9unyL"
      },
      "source": [
        "data = pickle.load(open(\"/content/drive/MyDrive/BBC Dataset/CNN Summarisation/embeddingsToSaliency1.pickle\", \"rb\"))\n",
        "#data2 = pickle.load(open(\"/content/drive/MyDrive/BBC Dataset/CNN Summarisation/embeddingsToSaliency2.pickle\", \"rb\"))\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq6XDO9kwNaU"
      },
      "source": [
        "#data = np.concatenate((pickle.load(open(\"/content/drive/MyDrive/BBC Dataset/CNN Summarisation/embeddingsToSaliency1.pickle\", \"rb\")),pickle.load(open(\"/content/drive/MyDrive/BBC Dataset/CNN Summarisation/embeddingsToSaliency2.pickle\", \"rb\"))), axis=0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWZbibwAzm_-"
      },
      "source": [
        "x = data[::2]\n",
        "y = data[1::2]\n",
        "del data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKzm_FXhzvDN",
        "outputId": "ec7b53cd-2d04-4e97-da70-7953483505f8"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "x = np.dstack(x)\n",
        "x = np.rollaxis(x, -1)\n",
        "x = np.expand_dims(x, axis=1)\n",
        "\n",
        "mask = y==-1\n",
        "\n",
        "print(\"removing -1s...\")\n",
        "x = x[~mask, :]\n",
        "y = y[~mask]\n",
        "\n",
        "print(\"data loaded.\")\n",
        "x, y = shuffle(x, y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "removing -1s...\n",
            "data loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtBltdrb-tOl"
      },
      "source": [
        "conv_window_size = (1, 300)\n",
        "num_filters = 100\n",
        "reg = 0.01\n",
        "dropout = 0.5\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 16\n",
        "val_train_ratio = 0.2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4RUuItCpXnkK",
        "outputId": "b086819d-d818-4920-a0d4-ab814014e198"
      },
      "source": [
        "!pip install tf-nightly"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/d6/44a05484afc803d4c39f141f598a3951aba636fe2c6f0e9930db56e9f4ff/tf_nightly-2.6.0.dev20210507-cp37-cp37m-manylinux2010_x86_64.whl (456.7MB)\n",
            "\u001b[K     |████████████████████████████████| 456.7MB 34kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.3.0)\n",
            "Collecting grpcio<2.0,>=1.37.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/d8/1bfe90cc49c166dd2ec1be46fa4830c254ce702004a110830c74ec1df0c0/grpcio-1.37.1-cp37-cp37m-manylinux2014_x86_64.whl (4.2MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2MB 45.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.12.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.7.4.3)\n",
            "Collecting tf-estimator-nightly~=2.5.0.dev\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/6c/9bf4a6004d18c8e543845d3416e50f36dd09d272161e2fb0db5678132dfd/tf_estimator_nightly-2.5.0.dev2021032601-py2.py3-none-any.whl (462kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 33.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.2)\n",
            "Collecting gast==0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl\n",
            "Collecting h5py~=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/74/9eae2bedd8201ab464308f42c601a12d79727a1c87f0c867fdefb212c6cf/h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.12.1)\n",
            "Collecting keras-nightly~=2.6.0.dev\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/38/8824c16d0bad4181f4c64b27a4c0d96c061e0317688af9afdad8643fb559/keras_nightly-2.6.0.dev2021050600-py2.py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 39.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.12.4)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.36.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.12)\n",
            "Collecting tb-nightly~=2.6.0.a\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/04/a70a057524517bfcc0c18fada432364de0b5c416ce994c93c7659456fd7f/tb_nightly-2.6.0a20210507-py3-none-any.whl (5.9MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9MB 48.1MB/s \n",
            "\u001b[?25hCollecting cached-property; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tf-nightly) (56.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (1.28.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (0.4.4)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f9/802efd84988bffd9f644c03b6e66fde8e76c3aa33db4279ddd11c5d61f4b/tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9MB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly~=2.6.0.a->tf-nightly) (3.10.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.6.0.a->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly~=2.6.0.a->tf-nightly) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.6.0.a->tf-nightly) (3.1.0)\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement grpcio~=1.32.0, but you'll have grpcio 1.37.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement h5py~=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: grpcio, tf-estimator-nightly, gast, cached-property, h5py, keras-nightly, tensorboard-data-server, tb-nightly, tf-nightly\n",
            "  Found existing installation: grpcio 1.32.0\n",
            "    Uninstalling grpcio-1.32.0:\n",
            "      Successfully uninstalled grpcio-1.32.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "Successfully installed cached-property-1.5.2 gast-0.4.0 grpcio-1.37.1 h5py-3.1.0 keras-nightly-2.6.0.dev2021050600 tb-nightly-2.6.0a20210507 tensorboard-data-server-0.6.1 tf-estimator-nightly-2.5.0.dev2021032601 tf-nightly-2.6.0.dev20210507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "grpc",
                  "h5py",
                  "keras",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVjoO88FFl1w"
      },
      "source": [
        "#import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Conv2D\n",
        "from keras import regularizers\n",
        "from keras.optimizers import Adadelta\n",
        "def build_model(input_shape, conv_window_size, num_filters, reg, dropout):\n",
        "    model = Sequential()\n",
        "    #model.add(Embedding(max_features,300))\n",
        "\n",
        "    # we add a Convolution 1D, which will learn num_filters\n",
        "    # word group filters of size conv_window_size:\n",
        "    model.add(Conv2D(input_shape=input_shape,\n",
        "                        filters=num_filters,\n",
        "                        kernel_size=conv_window_size,\n",
        "                        padding=\"same\",\n",
        "                        activation=\"relu\",\n",
        "                        strides=1,\n",
        "                        ))\n",
        "    \n",
        "    model.add(MaxPooling2D(pool_size=(num_filters, 1) ,padding=\"same\")  ) \n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1, activation='softmax', kernel_regularizer=regularizers.l2(reg)))\n",
        "    \n",
        "    #In addition, an l2−norm constraint of the weights w_r is imposed during training as well\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=Adadelta(),\n",
        "                  metrics=['mae'])\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VPaD53pGQzQ"
      },
      "source": [
        "def train(model, x_train, y_train, val_train_ratio=0.2, epochs=1000, batch_size=128):\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        validation_split=val_train_ratio,\n",
        "                        shuffle=False,\n",
        "                        verbose=1)\n",
        "    return history"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxFbcYmMJVHb"
      },
      "source": [
        "from keras import backend as K\n",
        "x = K.cast_to_floatx(x)\n",
        "y = K.cast_to_floatx(y)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EapJ6UNDSy6k",
        "outputId": "1c7743b5-92a5-4a47-a4a1-382cb2a7665a"
      },
      "source": [
        "print(x.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 1, 2134, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxsZBhGBOUt7",
        "outputId": "9610e1ee-a941-4907-c46b-c5344148fd97"
      },
      "source": [
        "import tensorflow as tf\n",
        "#images_nhwc = tf.placeholder(tf.float32, [None, 200, 300, 3])  # input batch\n",
        "out = tf.transpose(x, [0, 2, 3, 1])\n",
        "print(out.get_shape())\n",
        "x=out\n",
        "del out  # the shape of out is [None, 3, 200, 300]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 2134, 300, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7bRNNBsF7Wc",
        "outputId": "bd3f9c9a-129d-437e-f7c2-cfecdbb627e2"
      },
      "source": [
        "model = build_model(( x.shape[1], x.shape[2],1), conv_window_size, num_filters, reg, dropout)\n",
        "\n",
        "history = train(model, x, y, val_train_ratio, epochs, batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}