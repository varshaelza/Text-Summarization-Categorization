{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDASumm2.py",
      "provenance": [],
      "authorship_tag": "ABX9TyPjFWBR+Qo5zYlBd3n9r/YT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varshaelza/Text-Summarization-Categorization/blob/main/LDASumm2_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAyRzeJqSCJq"
      },
      "source": [
        "def sep(text):\n",
        "  import pandas as pd\n",
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "  import re\n",
        "  text= text.replace(\"\\t\",\" \")\n",
        "  text=re.sub(r'\\.+', '.',text)\n",
        "  k=0\n",
        "  c=0\n",
        "  p=0\n",
        "  a_list=[]\n",
        "  while k<len(text):\n",
        "    if c==0 and text[k]=='.':\n",
        "      a_list.append(text[p:k+1])\n",
        "      p=k+1\n",
        "    elif c==0 and text[k]=='\"':\n",
        "      c=1\n",
        "    elif c==1 and text[k]=='\"' and text[k-1]==\".\" :\n",
        "      a_list.append(text[p:k+1])\n",
        "      p=k+1\n",
        "      c=0\n",
        "    elif c==1 and text[k]=='\"' and text[k-2]==\".\" :\n",
        "      a_list.append(text[p:k+1])\n",
        "      p=k+1\n",
        "      c=0\n",
        "    elif c==1 and text[k]=='\"':\n",
        "      c=0\n",
        "    k=k+1\n",
        "  if p<len(text)-1:\n",
        "    a_list.append(text[p:len(text)])\n",
        " # a_list = nltk.tokenize.sent_tokenize(text)\n",
        "  #s=len(a_list)\n",
        "  df=pd.DataFrame()\n",
        "  print(a_list[0])\n",
        "  j=0\n",
        "  df['News']=\"\"\n",
        "  while j<len(a_list):\n",
        "    if j!=0:\n",
        "      df.loc[len(df.index)]=[a_list[j].replace(\"\\n\",\"\")]\n",
        "    else:\n",
        "      df.loc[len(df.index)]=[a_list[j]]\n",
        "    j=j+1\n",
        "  return df\n",
        "\n",
        "def spl(text):\n",
        "  t=text.split('\\n',1)\n",
        "  if len(t)!=2:\n",
        "    return \"\"\n",
        "  else :\n",
        "    return t[1]\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "#print(stopwords.words('english'))\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "  from nltk.corpus import stopwords\n",
        "  STOPWORDS = set(stopwords.words('english'))\n",
        "  \"\"\"custom function to remove the stopwords\"\"\"\n",
        "  return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "\n",
        "def lemmatize_words(text):\n",
        "  \n",
        "  from nltk.stem import WordNetLemmatizer\n",
        "  import nltk\n",
        "  nltk.download('wordnet')\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"custom function to remove the punctuation\"\"\"\n",
        "    import string\n",
        "    PUNCT_TO_REMOVE =  '''!()-[]{};:'\"\\,<>/?@#$%^&*_~'''\n",
        "    no_punct = \"\"\n",
        "    for char in text:\n",
        "      if char not in PUNCT_TO_REMOVE:\n",
        "        no_punct = no_punct + char\n",
        "      else:\n",
        "        no_punct = no_punct + ' '\n",
        "    return(no_punct)\n",
        "\n",
        "   \n",
        "def remove_stop(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    STOPWORDS = ['a' ,'b', 'c','d','e','f' ,'g' ,'h','i','j','k','l','m','n','o' ,'p' ,'q','r','s','t','u' ,'v' ,'w','x','y','z']\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "\n",
        "\n",
        "def pre(df):\n",
        "  df[\"News(wh)\"]=df['News']\n",
        "  df[\"News(wh)\"][0]=spl(df[\"News\"][0])\n",
        "  df[\"News(wh)\"][0]=spl(df[\"News(wh)\"][0])\n",
        "  df[\"News1\"] = df[\"News(wh)\"].str.lower()\n",
        "  df['News1'] = df['News1'].replace('\\n','.', regex=True)\n",
        "  df['News1'] = df['News1'].replace('\\t',' ', regex=True)\n",
        "  df['News1'] = df['News1'].replace(r'\\.+', \".\", regex=True)\n",
        "  df[\"News1\"] = df[\"News1\"].apply(lambda text: remove_stopwords(text))\n",
        "  df['News1'] = df['News1'].str.replace('(\\d*\\.\\d+)|(\\d+\\.[0-9 ]+)|(\\d)', '')\n",
        "  df['News1'] = df['News1'].apply(lambda text: lemmatize_words(text))\n",
        "  df['News1'] = df['News1'].apply(lambda text: remove_punctuation(text))\n",
        "  df['News1'] = df['News1'].replace(r'\\.+', \".\", regex=True)\n",
        "  df['News1'] = df['News1'].apply(lambda text: remove_stop(text))\n",
        "  return df\n",
        "  \n",
        "\n",
        "def lemmatization(texts, tags=['NOUN', 'ADJ','VERB','ADV']): # filter noun and adjective\n",
        "       import spacy\n",
        "       nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "       output = []\n",
        "       for sent in texts:\n",
        "             doc = nlp(\" \".join(sent)) \n",
        "             output.append([token.lemma_ for token in doc if token.pos_ in tags])\n",
        "       return output\n",
        "\n",
        "def preprocess(text):\n",
        "  import gensim\n",
        "  result = []\n",
        "  for token in gensim.utils.simple_preprocess(text):\n",
        "    result.append(token)\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "#for five category topic model \n",
        "\n",
        "def format_topics_sentencesf(ldamodel, corpus, texts,docs):\n",
        "    # Init output\n",
        "    import pandas as pd\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if prop_topic >=.1 :  # => topics to include\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords,docs['News(wh)'][i]]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Topic', 'Perc_Contribution', 'Topic_Keywords','Sentence']\n",
        "\n",
        "    return sent_topics_df\n",
        "\n",
        "\n",
        "\n",
        "def scoringf(df,k):\n",
        "  import gensim\n",
        "  import joblib\n",
        "  from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "  from gensim.utils import simple_preprocess\n",
        "  from gensim.parsing.preprocessing import STOPWORDS\n",
        "  processed_docs = df['News1'].map(preprocess)\n",
        "  processed_docs = processed_docs.values.tolist()\n",
        "  processed_docs=lemmatization(processed_docs)\n",
        "  if k==0:\n",
        "    dictionary=joblib.load('/content/drive/MyDrive/BBC Dataset/LDA Summarization/buss_dict.jl')\n",
        "    lda_model=joblib.load('/content/drive/MyDrive/BBC Dataset/LDA Summarization/buss_opttfidf_genlda.jl')\n",
        "  elif k==1:\n",
        "    dictionary=joblib.load('/content/drive/MyDrive/BBC Dataset/LDA Summarization/ent_dict.jl')\n",
        "    lda_model=joblib.load('/content/drive/MyDrive/BBC Dataset/LDA Summarization/ent_opttfidf_genlda.jl')\n",
        "  elif k==2:\n",
        "    dictionary=joblib.load('/content/drive/MyDrive/BBC Dataset/LDA Summarization/pol_dict.jl')\n",
        "    lda_model=joblib.load('/content/drive/MyDrive/BBC Dataset/LDA Summarization/pol_opttfidf_genlda.jl')\n",
        "  elif k==3:\n",
        "    dictionary=joblib.load('/content/drive/MyDrive/BBC Dataset/LDA Summarization/spo_dict.jl')\n",
        "    lda_model=joblib.load('/content/drive/MyDrive/BBC Dataset/LDA Summarization/spo_opttfidf_genlda.jl')\n",
        "  elif k==4:\n",
        "    dictionary=joblib.load('/content/drive/MyDrive/BBC Dataset/LDA Summarization/tech_dict.jl')\n",
        "    lda_model=joblib.load('/content/drive/MyDrive/BBC Dataset/LDA Summarization/tech_opttfidf_genlda.jl')\n",
        "  bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "  from gensim import corpora, models\n",
        "  tfidf = models.TfidfModel(bow_corpus)\n",
        "  corpus_tfidf = tfidf[bow_corpus]\n",
        "  df1 = format_topics_sentencesf(ldamodel=lda_model, corpus=corpus_tfidf, texts=processed_docs,docs=df)\n",
        "  return df1\n",
        "\n",
        "#for all category topic model \n",
        "\n",
        "def format_topics_sentences(ldamodel, corpus, texts,docs):\n",
        "    # Init output\n",
        "    import pandas as pd\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if prop_topic >=.1 :  # => topics to include\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords,docs['News(wh)'][i]]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Topic', 'Perc_Contribution', 'Topic_Keywords','Sentence']\n",
        "\n",
        "    return sent_topics_df\n",
        "\n",
        "def scoring(df):\n",
        "  import gensim\n",
        "  import joblib\n",
        "  from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "  from gensim.utils import simple_preprocess\n",
        "  from gensim.parsing.preprocessing import STOPWORDS\n",
        "  processed_docs = df['News1'].map(preprocess)\n",
        "  processed_docs = processed_docs.values.tolist()\n",
        "  processed_docs=lemmatization(processed_docs)\n",
        "  dictionary=joblib.load('/content/drive/MyDrive/BBC Dataset/LDA Summarization/all_dict.jl')\n",
        "  lda_model=joblib.load('/content/drive/MyDrive/BBC Dataset/LDA Summarization/all_opttfidf_genlda.jl')\n",
        " \n",
        "  bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "  from gensim import corpora, models\n",
        "  tfidf = models.TfidfModel(bow_corpus)\n",
        "  corpus_tfidf = tfidf[bow_corpus]\n",
        "  df1 = format_topics_sentences(ldamodel=lda_model, corpus=corpus_tfidf, texts=processed_docs,docs=df)\n",
        "  return df1\n",
        "\n",
        "\n",
        "\n",
        "#import pandas as pd\n",
        "#train = pd.read_csv('/content/drive/MyDrive/BBC Dataset/LDA Summarization/LDASummariestrain.csv')\n",
        "#train['Summary1(Imp)']=\"\"\n",
        "#train['Length1(Imp)']=0\n",
        "#i=0\n",
        "\n",
        "#while i< len(train):\n",
        "def getsummary(text,w):\n",
        "  import pandas as pd\n",
        "  df=sep(text)\n",
        "  df=pre(df)\n",
        "  df1=scoringf(df,w)\n",
        "  j=1;\n",
        "  s=df['News'][0].split('\\n',1)[0]\n",
        "  s=s+'\\n'\n",
        "  c=1\n",
        "  l=0\n",
        "  df1['Count']=0\n",
        "  df1['Diff']=-10000.000\n",
        "  while j<len(df1):\n",
        "    if df1['Sentence'][j]==df1['Sentence'][j-1]:\n",
        "      #print(j)\n",
        "      c=c+1\n",
        "    else:\n",
        "      #if c>3:\n",
        "       # print(df1['Sentence'][j-1])\n",
        "        #s=s+df1['Sentence'][j-1]\n",
        "        df1['Count'][j-1]=c\n",
        "        df1['Diff'][j-1]=float(df1['Perc_Contribution'][j-1]-df1['Perc_Contribution'][j-c])\n",
        "        c=1\n",
        "        #l=l+1\n",
        "    j=j+1\n",
        "\n",
        "  df1['Count'][j-1]=c \n",
        "  df1['Diff'][j-1]=float(df1['Perc_Contribution'][j-1]-df1['Perc_Contribution'][j-c])\n",
        "  \n",
        "  dup=df1.copy(deep=True)      \n",
        "\n",
        "  dup.sort_values(by=['Count','Diff'],inplace=True,ascending=False)\n",
        "  k=dup.head(5)\n",
        "  \n",
        "  p=0\n",
        "  lg=0\n",
        "  while l<5 and l<len(k):\n",
        "    try:\n",
        "      if k['Count'][p]!=1 and k['Count'][p]!=0:\n",
        "        s=s+k['Sentence'][p]\n",
        "        lg=lg+1\n",
        "      p=p+1\n",
        "      l=l+1\n",
        "    except:\n",
        "      p=p+1\n",
        "\n",
        "\n",
        "  \n",
        "  #train['Summary1(Imp)'][i]=s\n",
        "  #train['Length1(Imp)'][i]=lg\n",
        "  #i=i+1\n",
        "  return s\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}